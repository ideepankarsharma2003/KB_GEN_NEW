{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import math\n",
    "import torch\n",
    "import wikipedia\n",
    "from newspaper import Article, ArticleException\n",
    "from GoogleNews import GoogleNews\n",
    "import IPython\n",
    "from pyvis.network import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from keys import extract_all_passages\n",
    "def generate_context(query:str, num_urls:int):\n",
    "    \n",
    "    response= requests.post(extract_all_passages, json={\n",
    "                                                                                    \"query\": query,\n",
    "                                                                                    \"num_urls\": int(num_urls),\n",
    "                                                                                    } )\n",
    "    \n",
    "    if response.ok:\n",
    "        # d= eval(response.content)\n",
    "        paragrahs= json.loads(response.content.decode(\n",
    "                                                        'utf-8'\n",
    "                                                    ))['paragraphs']\n",
    "        return paragrahs\n",
    "        \n",
    "    else:\n",
    "        print(\"Couldn't get the response from the 'extract-all-passages'   ðŸ¥²\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations_from_model_output(text):\n",
    "    relations = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
    "    for token in text_replaced.split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'subject': subject.strip(),\n",
    "                    'verb': relation.strip(),\n",
    "                    'object': object_.strip()\n",
    "                })\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                relations.append({\n",
    "                    'subject': subject.strip(),\n",
    "                    'verb': relation.strip(),\n",
    "                    'object': object_.strip()\n",
    "                })\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        relations.append({\n",
    "            'subject': subject.strip(),\n",
    "            'verb': relation.strip(),\n",
    "            'object': object_.strip()\n",
    "        })\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.relations = []\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"subject\", \"verb\", \"object\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    # def add_relation(self, r):\n",
    "    #     if not self.exists_relation(r):\n",
    "    #         self.relations.append(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r1, r)][0]\n",
    "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"]\n",
    "                        if span not in r2[\"meta\"][\"spans\"]]\n",
    "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new class modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_text_to_kb(text, span_length=128, verbose=False):\n",
    "    # tokenize whole text\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
    "\n",
    "    # compute span boundaries\n",
    "    num_tokens = len(inputs[\"input_ids\"][0])\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_tokens} tokens\")\n",
    "    num_spans = math.ceil(num_tokens / span_length)\n",
    "    if verbose:\n",
    "        print(f\"Input has {num_spans} spans\")\n",
    "    overlap = math.ceil((num_spans * span_length - num_tokens) / \n",
    "                        max(num_spans - 1, 1))\n",
    "    spans_boundaries = []\n",
    "    start = 0\n",
    "    for i in range(num_spans):\n",
    "        spans_boundaries.append([start + span_length * i,\n",
    "                                 start + span_length * (i + 1)])\n",
    "        start -= overlap\n",
    "    if verbose:\n",
    "        print(f\"Span boundaries are {spans_boundaries}\")\n",
    "\n",
    "    # transform input with spans\n",
    "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
    "                  for boundary in spans_boundaries]\n",
    "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
    "                    for boundary in spans_boundaries]\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.stack(tensor_ids),\n",
    "        \"attention_mask\": torch.stack(tensor_masks)\n",
    "    }\n",
    "    inputs= transformers.tokenization_utils_base.BatchEncoding(inputs)\n",
    "\n",
    "    # generate relations\n",
    "    num_return_sequences = 3\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 256,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": num_return_sequences\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "\n",
    "    # decode relations\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
    "                                           skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    kb = KB()\n",
    "    i = 0\n",
    "    for sentence_pred in decoded_preds:\n",
    "        current_span_index = i // num_return_sequences\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for relation in relations:\n",
    "            relation[\"meta\"] = {\n",
    "                \"spans\": [spans_boundaries[current_span_index]]\n",
    "            }\n",
    "            kb.add_relation(relation)\n",
    "        i += 1\n",
    "    print(f\"relations generated: {len(kb.relations)}\")\n",
    "    # print(kb.relations)\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def from_text_to_kb(text, verbose=False):\n",
    "    kb = KB()\n",
    "\n",
    "    # Tokenizer text\n",
    "    model_inputs = tokenizer(text, max_length=512, padding=True, truncation=True,\n",
    "                            return_tensors='pt')\n",
    "    if verbose:\n",
    "        print(f\"Num tokens: {len(model_inputs['input_ids'][0])}\")\n",
    "\n",
    "    # Generate\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": 216,\n",
    "        \"length_penalty\": 0,\n",
    "        \"num_beams\": 3,\n",
    "        \"num_return_sequences\": 3\n",
    "    }\n",
    "    generated_tokens = model.generate(\n",
    "        **model_inputs.to(model.device),\n",
    "        **gen_kwargs,\n",
    "    )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "    # create kb\n",
    "    for sentence_pred in decoded_preds:\n",
    "        relations = extract_relations_from_model_output(sentence_pred)\n",
    "        for r in relations:\n",
    "            kb.add_relation(r)\n",
    "    print(f\"relations generated: {len(kb.relations)}\")\n",
    "    \n",
    "\n",
    "    return kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para= generate_context(\"best cat ear headphones\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb= KB()\n",
    "\n",
    "print(f\"***************  Total Paragraphs: {len(para)}  ***************\")\n",
    "\n",
    "i= 1\n",
    "for p in para:\n",
    "    print(f\"paragraph {i}:\")\n",
    "    i+=1\n",
    "    temp_kb= from_text_to_kb(p, verbose=True)\n",
    "    kb.relations+=(temp_kb.relations)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kb.relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(kb.relations, open(\"relation.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= {\n",
    "    'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 3\n",
    "}\n",
    "\n",
    "d2= {\n",
    "    'x': 1,\n",
    "    'y': 2,\n",
    "    'z': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.update(d2)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Filter and Normalize Entities: `Entity Linking`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "    def __init__(self):\n",
    "        self.entities = {}\n",
    "        self.relations = []\n",
    "\n",
    "\n",
    "\n",
    "    def are_relations_equal(self, r1, r2):\n",
    "        return all(r1[attr] == r2[attr] for attr in [\"subject\", \"verb\", \"object\"])\n",
    "\n",
    "    def exists_relation(self, r1):\n",
    "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
    "\n",
    "    # def add_relation(self, r):\n",
    "    #     if not self.exists_relation(r):\n",
    "    #         self.relations.append(r)\n",
    "\n",
    "    # def print(self):\n",
    "    #     print(\"Relations:\")\n",
    "    #     for r in self.relations:\n",
    "    #         print(f\"  {r}\")\n",
    "\n",
    "    def merge_relations(self, r1):\n",
    "        r2 = [r for r in self.relations\n",
    "              if self.are_relations_equal(r1, r)][0]\n",
    "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"]\n",
    "                        if span not in r2[\"meta\"][\"spans\"]]\n",
    "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
    "\n",
    "\n",
    "    def get_wikipedia_data(self, candidate_entity):\n",
    "        try:\n",
    "            page = wikipedia.page(candidate_entity, auto_suggest=False)\n",
    "            entity_data = {\n",
    "                \"title\": page.title,\n",
    "                \"url\": page.url,\n",
    "                \"summary\": page.summary\n",
    "            }\n",
    "            return entity_data\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def add_entity(self, e):\n",
    "        self.entities[e[\"title\"]] = {k:v for k,v in e.items() if k != \"title\"}\n",
    "\n",
    "    def add_relation(self, r):\n",
    "        # check on wikipedia\n",
    "        candidate_entities = [r[\"subject\"], r[\"object\"]]\n",
    "        entities = [self.get_wikipedia_data(ent) for ent in candidate_entities]\n",
    "\n",
    "        # if one entity does not exist, stop\n",
    "        if any(ent is None for ent in entities):\n",
    "            return\n",
    "\n",
    "        # manage new entities\n",
    "        for e in entities:\n",
    "            self.add_entity(e)\n",
    "\n",
    "        # rename relation entities with their wikipedia titles\n",
    "        r[\"subject\"] = entities[0][\"title\"]\n",
    "        r[\"object\"] = entities[1][\"title\"]\n",
    "\n",
    "        # manage new relation\n",
    "        if not self.exists_relation(r):\n",
    "            self.relations.append(r)\n",
    "        else:\n",
    "            self.merge_relations(r)\n",
    "\n",
    "    def print(self):\n",
    "        print(\"Entities:\")\n",
    "        for e in self.entities.items():\n",
    "            print(f\"  {e}\")\n",
    "        print(\"Relations:\")\n",
    "        for r in self.relations:\n",
    "            print(f\"  {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb= KB()\n",
    "\n",
    "print(f\"***************  Total Paragraphs: {len(para)}  ***************\")\n",
    "\n",
    "i= 1\n",
    "for p in para:\n",
    "    print(f\"paragraph {i}:\")\n",
    "    i+=1\n",
    "    temp_kb= from_text_to_kb(p, verbose=True)\n",
    "    kb.relations+=(temp_kb.relations)\n",
    "    print()\n",
    "    \n",
    "print(f\"***************  Total Relations: {len(kb.relations)}  ***************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(kb.relations, open(\"new-relation.json\", 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualizing the Knowledge Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_html(kb, filename=\"network.html\"):\n",
    "    # create network\n",
    "    net = Network(directed=True, width=\"700px\", height=\"700px\", bgcolor=\"#eeeeee\")\n",
    "\n",
    "    # nodes\n",
    "    color_entity = \"#0AFFF2\"\n",
    "    for e in kb.entities:\n",
    "        net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "    # edges\n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r[\"subject\"], r[\"object\"],\n",
    "                    title=r[\"verb\"], label=r[\"verb\"])\n",
    "        \n",
    "    # save network\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for i in kb.relations:\n",
    "    entities.append(i[\"subject\"])\n",
    "    entities.append(i[\"object\"])\n",
    "entities= set(entities)\n",
    "len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network_html(kb, filename=\"network.html\"):\n",
    "    # create network\n",
    "    net = Network(directed=True, width=\"700px\", height=\"700px\", bgcolor=\"#eeeeee\")\n",
    "\n",
    "    # nodes\n",
    "    color_entity = \"#0AFFF2\"\n",
    "    for e in entities:\n",
    "        net.add_node(e, shape=\"circle\", color=color_entity)\n",
    "\n",
    "    # edges\n",
    "    for r in kb.relations:\n",
    "        net.add_edge(r[\"subject\"], r[\"object\"],\n",
    "                    title=r[\"verb\"], label=r[\"verb\"])\n",
    "        \n",
    "    # save network\n",
    "    net.repulsion(\n",
    "        node_distance=200,\n",
    "        central_gravity=0.2,\n",
    "        spring_length=200,\n",
    "        spring_strength=0.05,\n",
    "        damping=0.09\n",
    "    )\n",
    "    net.set_edge_smooth('dynamic')\n",
    "    net.show(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# news_links = get_news_links(\"Google\", pages=5, max_links=20)\n",
    "# kb = from_urls_to_kb(news_links, verbose=True)\n",
    "filename = \"network_3_google.html\"\n",
    "save_network_html(kb, filename=filename)\n",
    "IPython.display.HTML(filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
